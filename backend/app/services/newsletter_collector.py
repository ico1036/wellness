"""
ë‰´ìŠ¤ë ˆí„° ìˆ˜ì§‘ ì„œë¹„ìŠ¤
RSS í”¼ë“œ ë° ì›¹ í¬ë¡¤ë§ì„ í†µí•œ ì›°ë‹ˆìŠ¤ ë¦¬íŠ¸ë¦¬íŠ¸ ë‰´ìŠ¤ë ˆí„° ìˆ˜ì§‘
"""
import asyncio
import hashlib
import logging
import time
from datetime import datetime, timedelta
from typing import List, Optional, Dict, Any
from urllib.parse import urljoin, urlparse
from urllib.robotparser import RobotFileParser

import aiofiles
import feedparser
import httpx
from bs4 import BeautifulSoup
from sqlalchemy.orm import Session

from app.core.config import settings
from app.models.newsletter import Newsletter, NewsletterSource, ContentQuality
from app.schemas.newsletter import NewsletterCreate, PrimaryCategoryEnum

logger = logging.getLogger(__name__)


class NewsletterCollector:
    """ë‰´ìŠ¤ë ˆí„° ìˆ˜ì§‘ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.session = None
        self.client = None
        self.collected_count = 0
        self.error_count = 0
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.client = httpx.AsyncClient(
            timeout=settings.REQUEST_TIMEOUT,
            headers={
                "User-Agent": settings.USER_AGENT,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate",
                "DNT": "1",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.client:
            await self.client.aclose()
    
    def check_robots_txt(self, base_url: str) -> bool:
        """
        robots.txt í™•ì¸
        ìœ¤ë¦¬ì  í¬ë¡¤ë§ì„ ìœ„í•œ robots.txt ì¤€ìˆ˜
        """
        try:
            rp = RobotFileParser()
            rp.set_url(f"{base_url}/robots.txt")
            rp.read()
            return rp.can_fetch(settings.USER_AGENT, base_url)
        except Exception as e:
            logger.warning(f"robots.txt í™•ì¸ ì‹¤íŒ¨: {base_url} - {e}")
            return True  # í™•ì¸ ì‹¤íŒ¨ ì‹œ í—ˆìš©ìœ¼ë¡œ ì²˜ë¦¬
    
    async def collect_from_rss(self, source: NewsletterSource, db: Session) -> List[Newsletter]:
        """
        RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ë ˆí„° ìˆ˜ì§‘
        """
        collected_newsletters = []
        
        try:
            logger.info(f"RSS ìˆ˜ì§‘ ì‹œì‘: {source.name} ({source.url})")
            
            # RSS í”¼ë“œ íŒŒì‹±
            feed = feedparser.parse(source.url)
            
            if feed.bozo:
                logger.warning(f"RSS í”¼ë“œ íŒŒì‹± ì˜¤ë¥˜: {source.name} - {feed.bozo_exception}")
            
            for entry in feed.entries:
                try:
                    # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                    title = entry.get('title', '').strip()
                    summary = entry.get('summary', entry.get('description', '')).strip()
                    content = entry.get('content', [{'value': summary}])[0].get('value', summary)
                    link = entry.get('link', '')
                    
                    # ë°œí–‰ì¼ ì²˜ë¦¬
                    published_date = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        published_date = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        published_date = datetime(*entry.updated_parsed[:6])
                    
                    # ì›°ë‹ˆìŠ¤ ë¦¬íŠ¸ë¦¬íŠ¸ ê´€ë ¨ì„± ê²€ì‚¬
                    if not self._is_wellness_retreat_related(title, content):
                        continue
                    
                    # ë‰´ìŠ¤ë ˆí„° ê°ì²´ ìƒì„±
                    newsletter_data = NewsletterCreate(
                        title=title,
                        summary=summary[:300] if summary else None,
                        content=content,
                        source=source.name,
                        source_url=link,
                        primary_category=source.default_category,
                        published_date=published_date
                    )
                    
                    newsletter = await self._create_newsletter(newsletter_data, db)
                    if newsletter:
                        collected_newsletters.append(newsletter)
                        self.collected_count += 1
                    
                    # ìš”ì²­ ê°„ê²© ì¤€ìˆ˜
                    await asyncio.sleep(settings.REQUEST_DELAY)
                    
                except Exception as e:
                    logger.error(f"RSS í•­ëª© ì²˜ë¦¬ ì˜¤ë¥˜: {source.name} - {e}")
                    self.error_count += 1
                    continue
            
            # ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê°„ ì—…ë°ì´íŠ¸
            source.last_collected = datetime.now()
            db.commit()
            
        except Exception as e:
            logger.error(f"RSS ìˆ˜ì§‘ ì‹¤íŒ¨: {source.name} - {e}")
            self.error_count += 1
        
        logger.info(f"RSS ìˆ˜ì§‘ ì™„ë£Œ: {source.name} - {len(collected_newsletters)}ê°œ ìˆ˜ì§‘")
        return collected_newsletters
    
    async def collect_from_web(self, source: NewsletterSource, db: Session) -> List[Newsletter]:
        """
        ì›¹ í¬ë¡¤ë§ì„ í†µí•œ ë‰´ìŠ¤ë ˆí„° ìˆ˜ì§‘
        """
        collected_newsletters = []
        
        try:
            # robots.txt í™•ì¸
            base_url = f"{urlparse(source.url).scheme}://{urlparse(source.url).netloc}"
            if not self.check_robots_txt(base_url):
                logger.warning(f"robots.txt ê±°ë¶€: {source.name}")
                return collected_newsletters
            
            logger.info(f"ì›¹ í¬ë¡¤ë§ ì‹œì‘: {source.name} ({source.url})")
            
            # ì›¹ í˜ì´ì§€ ìš”ì²­
            response = await self.client.get(source.url)
            response.raise_for_status()
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë‰´ìŠ¤ë ˆí„° í•­ëª© ì¶”ì¶œ (ì¼ë°˜ì ì¸ íŒ¨í„´ë“¤)
            newsletter_items = self._extract_newsletter_items(soup, source.url)
            
            for item in newsletter_items:
                try:
                    # ì›°ë‹ˆìŠ¤ ë¦¬íŠ¸ë¦¬íŠ¸ ê´€ë ¨ì„± ê²€ì‚¬
                    if not self._is_wellness_retreat_related(item['title'], item['content']):
                        continue
                    
                    newsletter_data = NewsletterCreate(
                        title=item['title'],
                        summary=item['summary'],
                        content=item['content'],
                        source=source.name,
                        source_url=item['url'],
                        primary_category=source.default_category,
                        published_date=item.get('published_date')
                    )
                    
                    newsletter = await self._create_newsletter(newsletter_data, db)
                    if newsletter:
                        collected_newsletters.append(newsletter)
                        self.collected_count += 1
                    
                    # ìš”ì²­ ê°„ê²© ì¤€ìˆ˜
                    await asyncio.sleep(settings.REQUEST_DELAY)
                    
                except Exception as e:
                    logger.error(f"ì›¹ í¬ë¡¤ë§ í•­ëª© ì²˜ë¦¬ ì˜¤ë¥˜: {source.name} - {e}")
                    self.error_count += 1
                    continue
            
            # ë§ˆì§€ë§‰ ìˆ˜ì§‘ ì‹œê°„ ì—…ë°ì´íŠ¸
            source.last_collected = datetime.now()
            db.commit()
            
        except Exception as e:
            logger.error(f"ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨: {source.name} - {e}")
            self.error_count += 1
        
        logger.info(f"ì›¹ í¬ë¡¤ë§ ì™„ë£Œ: {source.name} - {len(collected_newsletters)}ê°œ ìˆ˜ì§‘")
        return collected_newsletters
    
    def _extract_newsletter_items(self, soup: BeautifulSoup, base_url: str) -> List[Dict[str, Any]]:
        """
        ì›¹ í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ë ˆí„° í•­ëª© ì¶”ì¶œ
        ë‹¤ì–‘í•œ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ëŒ€ì‘í•˜ëŠ” ì¼ë°˜ì ì¸ ì¶”ì¶œ ë¡œì§
        """
        items = []
        
        # ì¼ë°˜ì ì¸ ë‰´ìŠ¤ë ˆí„°/ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸ ì…€ë ‰í„°ë“¤
        selectors = [
            'article',
            '.post', '.blog-post', '.news-item',
            '.entry', '.story', '.content-item',
            'h2, h3'  # ì œëª©ë§Œ ìˆëŠ” ê²½ìš°
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            if elements and len(elements) > 1:  # ì¶©ë¶„í•œ í•­ëª©ì´ ìˆëŠ” ì…€ë ‰í„° ì„ íƒ
                for element in elements[:10]:  # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ
                    item = self._extract_item_from_element(element, base_url)
                    if item and len(item['title']) > 10:  # ìœ íš¨í•œ ì œëª©ì´ ìˆëŠ” ê²½ìš°ë§Œ
                        items.append(item)
                break
        
        return items
    
    def _extract_item_from_element(self, element, base_url: str) -> Optional[Dict[str, Any]]:
        """HTML ìš”ì†Œì—ì„œ ë‰´ìŠ¤ë ˆí„° ì •ë³´ ì¶”ì¶œ"""
        try:
            # ì œëª© ì¶”ì¶œ
            title_elem = element.find(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            title = title_elem.get_text(strip=True) if title_elem else element.get_text(strip=True)[:100]
            
            if not title:
                return None
            
            # ë§í¬ ì¶”ì¶œ
            link_elem = element.find('a', href=True)
            url = ''
            if link_elem:
                href = link_elem['href']
                url = urljoin(base_url, href) if not href.startswith('http') else href
            
            # ë‚´ìš© ì¶”ì¶œ
            content = element.get_text(strip=True)
            
            # ìš”ì•½ ìƒì„± (ì²« 300ì)
            summary = content[:300] + '...' if len(content) > 300 else content
            
            return {
                'title': title,
                'content': content,
                'summary': summary,
                'url': url,
                'published_date': None  # ì¶”í›„ ê°œì„  ê°€ëŠ¥
            }
            
        except Exception as e:
            logger.error(f"í•­ëª© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None
    
    def _is_wellness_retreat_related(self, title: str, content: str) -> bool:
        """
        ì›°ë‹ˆìŠ¤ ë¦¬íŠ¸ë¦¬íŠ¸ ê´€ë ¨ì„± ê²€ì‚¬
        Writerê°€ ì •ì˜í•œ í‚¤ì›Œë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ì„± íŒë‹¨
        """
        # ì›°ë‹ˆìŠ¤ ë¦¬íŠ¸ë¦¬íŠ¸ ê´€ë ¨ í‚¤ì›Œë“œë“¤
        wellness_keywords = [
            # í•œêµ­ì–´ í‚¤ì›Œë“œ
            'ì›°ë‹ˆìŠ¤', 'ë¦¬íŠ¸ë¦¬íŠ¸', 'íë§', 'ëª…ìƒ', 'ìš”ê°€', 'ìŠ¤íŒŒ', 'ë””í†¡ìŠ¤',
            'ë§ˆìŒìˆ˜ë ¨', 'ì‹¬ì‹ ', 'ì¹˜ìœ ', 'íœ´ì–‘', 'ê±´ê°•ì—¬í–‰', 'í…œí”ŒìŠ¤í…Œì´',
            'ì‚°ë¦¼ìš•', 'ìì—°ì¹˜ìœ ', 'ì•„ë¡œë§ˆ', 'ë§ˆì‚¬ì§€', 'í•„ë¼í…ŒìŠ¤',
            
            # ì˜ì–´ í‚¤ì›Œë“œ  
            'wellness', 'retreat', 'healing', 'meditation', 'yoga', 'spa', 'detox',
            'mindfulness', 'holistic', 'therapeutic', 'rejuvenation', 'restoration',
            'ayurveda', 'aromatherapy', 'pilates', 'fitness retreat'
        ]
        
        text = f"{title} {content}".lower()
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        keyword_count = sum(1 for keyword in wellness_keywords if keyword.lower() in text)
        
        # ìµœì†Œ 1ê°œ ì´ìƒì˜ í‚¤ì›Œë“œê°€ ìˆì–´ì•¼ í•¨
        return keyword_count > 0
    
    async def _create_newsletter(self, newsletter_data: NewsletterCreate, db: Session) -> Optional[Newsletter]:
        """
        ë‰´ìŠ¤ë ˆí„° ìƒì„± ë° ì¤‘ë³µ ê²€ì‚¬
        """
        try:
            # ì»¨í…ì¸  í•´ì‹œ ìƒì„± (ì¤‘ë³µ ê²€ì‚¬ìš©)
            content_hash = hashlib.sha256(
                f"{newsletter_data.title}{newsletter_data.content}".encode('utf-8')
            ).hexdigest()
            
            # ì¤‘ë³µ ê²€ì‚¬ (ìµœê·¼ 30ì¼ ë‚´ì—ì„œë§Œ ì²´í¬)
            cutoff_date = datetime.now() - timedelta(days=30)
            existing = db.query(Newsletter).filter(
                Newsletter.content_hash == content_hash,
                Newsletter.collected_date >= cutoff_date
            ).first()
            if existing:
                logger.debug(f"ì¤‘ë³µ ë‰´ìŠ¤ë ˆí„° ìŠ¤í‚µ (30ì¼ ë‚´): {newsletter_data.title[:50]}")
                return None
            
            # ê³ ìœ  ID ìƒì„±
            newsletter_id = f"nl_{int(time.time())}_{hash(newsletter_data.title) % 10000}"
            
            # ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜
            category = self._classify_category(newsletter_data.title, newsletter_data.content)
            
            # ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ
            location = self._extract_location(newsletter_data.title, newsletter_data.content)
            
            # í”„ë¡œê·¸ë¨ ì •ë³´ ì¶”ì¶œ
            program_info = self._extract_program_info(newsletter_data.title, newsletter_data.content)
            
            # Newsletter ê°ì²´ ìƒì„±
            newsletter = Newsletter(
                id=newsletter_id,
                title=newsletter_data.title,
                summary=newsletter_data.summary,
                content=newsletter_data.content,
                source=newsletter_data.source,
                source_url=newsletter_data.source_url,
                primary_category=category or newsletter_data.primary_category,
                tags=newsletter_data.tags or [],
                location=location,
                program_info=program_info,
                published_date=newsletter_data.published_date,
                content_hash=content_hash,
                quality_score=0.0  # ì¶”í›„ í’ˆì§ˆ í‰ê°€ì—ì„œ ì„¤ì •
            )
            
            db.add(newsletter)
            
            # í’ˆì§ˆ í‰ê°€ ì‹¤í–‰
            quality_score = await self._evaluate_quality(newsletter)
            newsletter.quality_score = quality_score
            
            db.commit()
            
            logger.info(f"ë‰´ìŠ¤ë ˆí„° ìƒì„±: {newsletter.title[:50]} (í’ˆì§ˆ: {quality_score:.2f})")
            return newsletter
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ë ˆí„° ìƒì„± ì˜¤ë¥˜: {e}")
            db.rollback()
            return None
    
    def _classify_category(self, title: str, content: str) -> Optional[PrimaryCategoryEnum]:
        """
        ë‹¨ìˆœí™”ëœ 3ê°œ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜ (ì—ì´ì „íŠ¸ íšŒì˜ ê²°ê³¼)
        """
        text = f"{title} {content}".lower()
        
        # ë‹¨ìˆœí™”ëœ 3ê°œ ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ë§¤í•‘
        category_keywords = {
            PrimaryCategoryEnum.MIND_WELLNESS: [
                # ğŸ§˜ ë§ˆìŒ ì›°ë‹ˆìŠ¤: ëª…ìƒ, ë§ˆì¸ë“œí’€ë‹ˆìŠ¤, ì •ì‹ ê±´ê°•
                'meditation', 'mindfulness', 'mental', 'mind', 'stress', 'therapy', 'spiritual',
                'ëª…ìƒ', 'ë§ˆìŒ', 'ì •ì‹ ', 'ìŠ¤íŠ¸ë ˆìŠ¤', 'ì‹¬ë¦¬', 'ì¹˜ìœ ', 'ë§ˆì¸ë“œí’€ë‹ˆìŠ¤'
            ],
            PrimaryCategoryEnum.BODY_WELLNESS: [
                # ğŸ’ª ëª¸ ì›°ë‹ˆìŠ¤: ìš”ê°€, ìš´ë™, ì˜ì–‘ (ê¸°ì¡´ nutrition í¬í•¨)
                'yoga', 'fitness', 'pilates', 'exercise', 'health', 'nutrition', 'diet', 'detox',
                'ìš”ê°€', 'ìš´ë™', 'í•„ë¼í…ŒìŠ¤', 'ê±´ê°•', 'ì˜ì–‘', 'ë‹¤ì´ì–´íŠ¸', 'í”¼íŠ¸ë‹ˆìŠ¤', 'ë””í†¡ìŠ¤', 'ê±´ê°•ì‹'
            ],
            PrimaryCategoryEnum.SPA_THERAPY: [
                # ğŸŒ¿ ìŠ¤íŒŒ & íë§: ìŠ¤íŒŒ, ë§ˆì‚¬ì§€, ìì—°ì¹˜ìœ  (ê¸°ì¡´ nature_healing í¬í•¨)
                'spa', 'massage', 'aromatherapy', 'healing', 'hot tub', 'relaxation', 'nature', 'forest',
                'ìŠ¤íŒŒ', 'ë§ˆì‚¬ì§€', 'ì•„ë¡œë§ˆ', 'íë§', 'ì˜¨ì²œ', 'íœ´ì‹', 'í…Œë¼í”¼', 'ìì—°', 'ì‚°ë¦¼ìš•', 'ìì—°ì¹˜ìœ '
            ]
        }
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚°
        category_scores = {}
        for category, keywords in category_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text)
            if score > 0:
                category_scores[category] = score
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ì¹´í…Œê³ ë¦¬ ë°˜í™˜
        if category_scores:
            return max(category_scores, key=category_scores.get)
        
        # ê¸°ë³¸ê°’: mind_wellness
        return PrimaryCategoryEnum.MIND_WELLNESS
    
    def _extract_location(self, title: str, content: str) -> Optional[Dict[str, Any]]:
        """ì»¨í…ì¸ ì—ì„œ ìœ„ì¹˜ ì •ë³´ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ ìœ„ì¹˜ ì¶”ì¶œ ë¡œì§ (ì¶”í›„ NLPë¡œ ê°œì„  ê°€ëŠ¥)
        text = f"{title} {content}".lower()
        
        # êµ­ê°€ í‚¤ì›Œë“œ
        countries = {
            'í•œêµ­': ['í•œêµ­', 'êµ­ë‚´', 'ì„œìš¸', 'ë¶€ì‚°', 'ì œì£¼'],
            'íƒœêµ­': ['íƒœêµ­', 'thailand', 'ë°©ì½•', 'í‘¸ì¼“'],
            'ì¸ë„ë„¤ì‹œì•„': ['ë°œë¦¬', 'bali', 'ì¸ë„ë„¤ì‹œì•„'],
            'ì¸ë„': ['ì¸ë„', 'india', 'ë¦¬ì‹œì¼€ì‹œ'],
            'ì¼ë³¸': ['ì¼ë³¸', 'japan', 'ë„ì¿„', 'ì˜¤í‚¤ë‚˜ì™€']
        }
        
        for country, keywords in countries.items():
            if any(keyword in text for keyword in keywords):
                return {
                    'country': country,
                    'region': None,
                    'specific': None
                }
        
        return None
    
    def _extract_program_info(self, title: str, content: str) -> Optional[Dict[str, Any]]:
        """í”„ë¡œê·¸ë¨ ì •ë³´ ì¶”ì¶œ"""
        text = f"{title} {content}".lower()
        program_info = {}
        
        # ê¸°ê°„ ì¶”ì¶œ
        if any(word in text for word in ['ë‹¹ì¼', '1ì¼', 'day trip']):
            program_info['duration'] = 'ë‹¹ì¼'
        elif any(word in text for word in ['1ë°•', '2ì¼', '1night']):
            program_info['duration'] = '1ë°•2ì¼'
        elif any(word in text for word in ['2ë°•', '3ì¼']):
            program_info['duration'] = '2-3ì¼'
        elif any(word in text for word in ['1ì£¼', 'week', '7ì¼']):
            program_info['duration'] = '1ì£¼ì¼'
        
        # ê°€ê²©ëŒ€ ì¶”ì¶œ (ë§¤ìš° ê°„ë‹¨í•œ ë¡œì§)
        if any(word in text for word in ['ì €ë ´', 'í•©ë¦¬ì ', 'affordable']):
            program_info['price_range'] = 'ì €ê°€'
        elif any(word in text for word in ['ëŸ­ì…”ë¦¬', 'luxury', 'í”„ë¦¬ë¯¸ì—„']):
            program_info['price_range'] = 'ëŸ­ì…”ë¦¬'
        elif any(word in text for word in ['ê³ ê¸‰', 'ê³ ê°€', 'expensive']):
            program_info['price_range'] = 'ê³ ê°€'
        
        return program_info if program_info else None
    
    async def _evaluate_quality(self, newsletter: Newsletter) -> float:
        """
        ë‰´ìŠ¤ë ˆí„° í’ˆì§ˆ í‰ê°€
        Writerê°€ ì •ì˜í•œ í’ˆì§ˆ ê¸°ì¤€ ì ìš©
        """
        score = 0.0
        max_score = 5.0
        
        # 1. ìœ„ì¹˜ ì •ë³´ ì¡´ì¬ (1ì )
        if newsletter.location:
            score += 1.0
        
        # 2. í”„ë¡œê·¸ë¨ ì •ë³´ ì¡´ì¬ (1ì )
        if newsletter.program_info:
            score += 1.0
        
        # 3. ì»¨í…ì¸  ê¸¸ì´ (1ì )
        content_length = len(newsletter.content)
        if content_length > 1000:
            score += 1.0
        elif content_length > 500:
            score += 0.5
        
        # 4. ì œëª© í’ˆì§ˆ (1ì )
        if len(newsletter.title) > 10 and any(keyword in newsletter.title.lower() 
                                             for keyword in ['ì›°ë‹ˆìŠ¤', 'wellness', 'ë¦¬íŠ¸ë¦¬íŠ¸', 'retreat']):
            score += 1.0
        
        # 5. ìš”ì•½ ì¡´ì¬ (1ì )
        if newsletter.summary and len(newsletter.summary) > 50:
            score += 1.0
        
        return min(score / max_score, 1.0)  # 0.0 ~ 1.0 ë²”ìœ„ë¡œ ì •ê·œí™”


class CollectionScheduler:
    """ë‰´ìŠ¤ë ˆí„° ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬"""
    
    def __init__(self, db: Session):
        self.db = db
    
    async def run_collection(self) -> Dict[str, Any]:
        """
        ì „ì²´ ë‰´ìŠ¤ë ˆí„° ìˆ˜ì§‘ ì‹¤í–‰
        """
        start_time = datetime.now()
        total_collected = 0
        total_errors = 0
        
        # í™œì„±í™”ëœ ì†ŒìŠ¤ë“¤ ê°€ì ¸ì˜¤ê¸°
        sources = self.db.query(NewsletterSource).filter(
            NewsletterSource.is_active == True
        ).all()
        
        logger.info(f"ë‰´ìŠ¤ë ˆí„° ìˆ˜ì§‘ ì‹œì‘: {len(sources)}ê°œ ì†ŒìŠ¤")
        
        async with NewsletterCollector() as collector:
            for source in sources:
                try:
                    if source.type == 'rss':
                        newsletters = await collector.collect_from_rss(source, self.db)
                    elif source.type == 'web':
                        newsletters = await collector.collect_from_web(source, self.db)
                    else:
                        logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì†ŒìŠ¤ íƒ€ì…: {source.type}")
                        continue
                    
                    total_collected += len(newsletters)
                    
                except Exception as e:
                    logger.error(f"ì†ŒìŠ¤ ìˆ˜ì§‘ ì˜¤ë¥˜: {source.name} - {e}")
                    total_errors += 1
            
            total_collected = collector.collected_count
            total_errors = collector.error_count
        
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        result = {
            'start_time': start_time,
            'end_time': end_time,
            'duration_seconds': duration,
            'sources_processed': len(sources),
            'newsletters_collected': total_collected,
            'errors': total_errors,
            'success_rate': (len(sources) - total_errors) / len(sources) if sources else 0
        }
        
        logger.info(f"ìˆ˜ì§‘ ì™„ë£Œ: {total_collected}ê°œ ìˆ˜ì§‘, {total_errors}ê°œ ì˜¤ë¥˜, {duration:.1f}ì´ˆ ì†Œìš”")
        return result